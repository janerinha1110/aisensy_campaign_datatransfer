name: Daily Campaign Scraper

on:
  schedule:
    # Run daily at 12:05 AM IST (6:35 PM UTC)
    - cron: '35 18 * * *'
  workflow_dispatch: # Allow manual trigger

jobs:
  scrape-campaigns:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
        
    - name: Install dependencies
      run: npm ci
      
    - name: Install Playwright browsers
      run: npx playwright install chromium --with-deps
      
    - name: Run campaign scraper
      env:
        EMAIL: ${{ secrets.EMAIL }}
        PASSWORD: ${{ secrets.PASSWORD }}
        LOGIN_URL: ${{ secrets.LOGIN_URL }}
        ASSISTANT_ID: ${{ secrets.ASSISTANT_ID }}
        SLACK_BOT_TOKEN: ${{ secrets.SLACK_BOT_TOKEN }}
        SLACK_CHANNEL_ID: ${{ secrets.SLACK_CHANNEL_ID }}
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
        NODE_ENV: production
      run: |
        # Run the scraper
        node index.js &
        SCRAPER_PID=$!
        
        # Wait for the server to start
        sleep 10
        
        # Trigger the cron check endpoint
        curl -X GET "http://localhost:3000/api/cron-check" || echo "Cron check completed"
        
        # Wait a bit for processing
        sleep 30
        
        # Stop the server
        kill $SCRAPER_PID || true
        
    - name: Upload logs (if any)
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: scraper-logs
        path: |
          *.log
          *.png
          *.json
          *.csv
        retention-days: 7 